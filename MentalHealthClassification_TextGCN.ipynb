{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install --user flair   #Needs restart on colab\n",
        "# !pip install transformers\n",
        "# !pip install --quiet optuna"
      ],
      "metadata": {
        "id": "7XWxRMO3yS1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "from tabulate import tabulate\n",
        "\n",
        "import torch"
      ],
      "metadata": {
        "id": "kUU7Of1XG-eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "id": "zRPLfBp-a0dX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "Kotzk_WfhXt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DATA\n",
        "original_train_sentences = []\n",
        "original_train_labels = []\n",
        "original_test_sentences = []\n",
        "original_test_labels = []\n",
        "\n",
        "#PREPROCESSING\n",
        "DEIDENTIFY = True     #True -> replaces URLs, emails, and usernames with reserved tokens\n",
        "EMOPRESERVE = True    #True -> adds emoticons and emojis to tokenizer vocabulary and prevents them from being affected by further text cleaning\n",
        "TEXTCLEAN = False     #True -> removes or isolates specific punctuations and expands contractions\n",
        "TOKEN_TYPE = \"wp\"     #wp -> wordpiece tokenization; ws -> word split\n",
        "\n",
        "#MMEMOG\n",
        "mmemogFile = \"_OUTPUT/MMEMOG_WordPieceEmbeddings_EmoLex.pkl\"\n",
        "\n",
        "#GCN Parameters\n",
        "EDGE = 2 # 0:d2w 1:d2w+w2w 2:d2w+w2w+d2d\n",
        "NODE = 3 # 0:one-hot #1:BERT #2:Glove #3:Custom(MMEMOG)\n",
        "EARLY_STOPPING = 10\n",
        "NUM_EPOCHS = 200\n",
        "\n",
        "#Save Paths\n",
        "session_name = \"TestModel\"\n",
        "save_output_path = \"_OUTPUT/\" + session_name + \".csv\" #Predictions"
      ],
      "metadata": {
        "id": "yXJu09VmhaPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Resources"
      ],
      "metadata": {
        "id": "f3QSsuKsDaz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#LOAD GLOVE\n",
        "gloveFileName = \"resources/glove.twitter.27B.100d.txt\"\n",
        "\n",
        "def getGloveModel():\n",
        "\n",
        "  #Convert Glove format to Word2Vec format\n",
        "  import gensim\n",
        "  from gensim.test.utils import datapath, get_tmpfile\n",
        "  from gensim.models import KeyedVectors\n",
        "  from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "  # https://radimrehurek.com/gensim/scripts/glove2word2vec.html\n",
        "  tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
        "  glove2word2vec(gloveFileName, tmp_file)\n",
        "  print(\"Converted glove to word2vec format\")\n",
        "\n",
        "  gloveModel = KeyedVectors.load_word2vec_format(tmp_file)\n",
        "  gloveDim = gloveModel.vector_size\n",
        "  print(\"Loaded pretrained glove model\")\n",
        "\n",
        "  return gloveModel, gloveDim"
      ],
      "metadata": {
        "id": "7K2wRVD_K7CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load list of emoticons\n",
        "with open(\"resources/TextEmoticonList.txt\", \"r\") as file:\n",
        "  emoticonList = file.read().split(\"\\n\")\n",
        "\n",
        "#Remove emoticons with spaces in-between\n",
        "emoticonList = [emoticon for emoticon in emoticonList if len(emoticon.split(\" \")) == 1]\n",
        "\n",
        "#Remove one character emoticons\n",
        "emoticonList = [emoticon for emoticon in emoticonList if len(emoticon) > 1]\n",
        "\n",
        "print(len(emoticonList))\n",
        "print(emoticonList[:10])"
      ],
      "metadata": {
        "id": "UKfgCH5aDQuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load list of emojis\n",
        "emojiList = pd.read_csv(\"resources/Emojis-Grid view.csv\")\n",
        "emojiList = emojiList[emojiList[\"Emoji\"] != \"C\"]\n",
        "emojiList = emojiList[\"Emoji\"].tolist()\n",
        "\n",
        "#Unicode versions\n",
        "emojiList_uni = [emoji.encode('unicode-escape').decode('ASCII') for emoji in emojiList]\n",
        "\n",
        "print(len(emojiList))\n",
        "print(emojiList[:10])\n",
        "print(emojiList_uni[:10])"
      ],
      "metadata": {
        "id": "Knh2n_bAZOrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJPI-IXrBkrP"
      },
      "source": [
        "# Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GyzNkI7W03D"
      },
      "outputs": [],
      "source": [
        "train_size = len(original_train_sentences)\n",
        "test_size = len(original_test_sentences)\n",
        "sentences = original_train_sentences + original_test_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2W7wKTBfa71"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenURL = \"_URL_\"\n",
        "tokenEmail = \"_EMAIL_\"\n",
        "tokenUsername = \"_USER_\"\n",
        "reserveTokens = [tokenURL, tokenEmail, tokenUsername]\n",
        "\n",
        "#CLEANING PROCESS\n",
        "#- Include emojis and emoticons\n",
        "#- Replace url, email, and usernames with tokens\n",
        "#- Remove non-major puncutations and separate them from words with whitespaces\n",
        "#- Lowercase\n",
        "def preprocess_str(string):\n",
        "\n",
        "  #Preclean\n",
        "  if DEIDENTIFY:\n",
        "    string = re.sub(r\"https?://[^\\s]+\", tokenURL, string)              #Links\n",
        "    string = re.sub(r\"[\\w.+-]+@[\\w-]+\\.[\\w.-]+\", tokenEmail, string)   #Email\n",
        "    string = re.sub(r\"@[a-zA-Z0-9_]{2,}\", tokenUsername, string)       #Usernames\n",
        "\n",
        "  #Emoticon/Emoji split\n",
        "  tokens = [string]\n",
        "  if EMOPRESERVE:\n",
        "    allEmo = emoticonList + emojiList + emojiList_uni + reserveTokens\n",
        "    for emoticon in allEmo:\n",
        "      if emoticon in string:\n",
        "        splits = []\n",
        "        for split in tokens:\n",
        "          # splits.append(re.split(r\"((^|\\s)\" + re.escape(emoticon) + \"(\\s|$))\", split))\n",
        "          splits.append(re.split(r\"(\" + re.escape(emoticon) + \")\", split))\n",
        "        tokens = [y.strip() for x in splits for y in x if y != \"\"]\n",
        "\n",
        "  for idx in range(len(tokens)):\n",
        "    if EMOPRESERVE and tokens[idx] in allEmo: #Skip emoticons, emojis\n",
        "      continue\n",
        "\n",
        "    if TEXTCLEAN:\n",
        "      tokens[idx] = re.sub(r\"[^A-Za-z0-9(),!?\\.\\'\\`]\", \" \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\'s\", \" \\'s\", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\'ve\", \" \\'ve\", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"n\\'t\", \" n\\'t\", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\'re\", \" \\'re\", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\'d\", \" \\'d\", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\'ll\", \" \\'ll\", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\",\", \" , \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"!\", \" ! \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\(\", \" ( \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\)\", \" ) \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\?\", \" ? \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\.\", \" . \", tokens[idx])\n",
        "      tokens[idx] = re.sub(r\"\\s{2,}\", \" \", tokens[idx])\n",
        "\n",
        "    #Lower case and strip by default\n",
        "    tokens[idx] = tokens[idx].lower().strip()\n",
        "\n",
        "  return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "-CFuPPcWFk-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenizer"
      ],
      "metadata": {
        "id": "eyfCxMUJeEQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "tokenizer.add_tokens(reserveTokens)\n",
        "tokenizer.add_tokens(emoticonList + emojiList + emojiList_uni)"
      ],
      "metadata": {
        "id": "mg8OoA7JeGTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMkEBxr6fMQi"
      },
      "source": [
        "## Remove Stopwords and less frequent words, tokenize sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xRG94uDfaBV"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "remove_limit = 5\n",
        "\n",
        "original_word_freq = {}  # to remove rare words\n",
        "for sentence in tqdm(sentences):\n",
        "    temp = preprocess_str(sentence)\n",
        "    word_list = tokenizer.tokenize(temp)[:512] #Use BertTokenizer; NOTE: manual truncation\n",
        "    for word in word_list:\n",
        "        if word in original_word_freq:\n",
        "            original_word_freq[word] += 1\n",
        "        else:\n",
        "            original_word_freq[word] = 1\n",
        "\n",
        "tokenize_sentences = []\n",
        "word_list_dict = {}\n",
        "for sentence in tqdm(sentences):\n",
        "    temp = preprocess_str(sentence)\n",
        "    word_list_temp = tokenizer.tokenize(temp)[:512] #Use BertTokenizer; NOTE: manual truncation\n",
        "    doc_words = []\n",
        "    for word in word_list_temp:\n",
        "        #NOTE: Including stopwords\n",
        "        # if word in original_word_freq and word not in stop_words and original_word_freq[word] >= remove_limit:\n",
        "        if word in original_word_freq and original_word_freq[word] >= remove_limit:\n",
        "            doc_words.append(word)\n",
        "            word_list_dict[word] = 1\n",
        "    tokenize_sentences.append(doc_words)\n",
        "word_list = list(word_list_dict.keys())\n",
        "vocab_length = len(word_list)\n",
        "\n",
        "#word to id dict\n",
        "word_id_map = {}\n",
        "for i in range(vocab_length):\n",
        "    word_id_map[word_list[i]] = i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqLUncB2Pn_L"
      },
      "outputs": [],
      "source": [
        "node_size = train_size + vocab_length + test_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hobYcJ5OX5oT"
      },
      "source": [
        "## Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtWyhXiueMOq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "unique_labels=np.unique(original_train_labels)\n",
        "\n",
        "num_class = len(unique_labels)\n",
        "lEnc = LabelEncoder()\n",
        "lEnc.fit(unique_labels)\n",
        "\n",
        "print(unique_labels)\n",
        "print(lEnc.transform(unique_labels))\n",
        "\n",
        "train_labels = lEnc.transform(original_train_labels)\n",
        "test_labels = lEnc.transform(original_test_labels)\n",
        "\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "labels = train_labels.tolist()+test_labels.tolist()\n",
        "labels = torch.LongTensor(labels).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0o8wcXgrTiD"
      },
      "source": [
        "# Model input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZbRV2wYxY1U"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znJ7Grz7fQ2L"
      },
      "source": [
        "## Build Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BSg1uNgV3_7"
      },
      "outputs": [],
      "source": [
        "from math import log\n",
        "row = []\n",
        "col = []\n",
        "weight = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QESQPT88AqsI"
      },
      "source": [
        "### word-word: PMI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNlJoLFagXhv"
      },
      "outputs": [],
      "source": [
        "if EDGE >= 1:\n",
        "    window_size = 20\n",
        "    total_W = 0\n",
        "    word_occurrence = {}\n",
        "    word_pair_occurrence = {}\n",
        "\n",
        "    def ordered_word_pair(a, b):\n",
        "        if a > b:\n",
        "            return b, a\n",
        "        else:\n",
        "            return a, b\n",
        "\n",
        "    def update_word_and_word_pair_occurrence(q):\n",
        "        unique_q = list(set(q))\n",
        "        for i in unique_q:\n",
        "            try:\n",
        "                word_occurrence[i] += 1\n",
        "            except:\n",
        "                word_occurrence[i] = 1\n",
        "        for i in range(len(unique_q)):\n",
        "            for j in range(i+1, len(unique_q)):\n",
        "                word1 = unique_q[i]\n",
        "                word2 = unique_q[j]\n",
        "                word1, word2 = ordered_word_pair(word1, word2)\n",
        "                try:\n",
        "                    word_pair_occurrence[(word1, word2)] += 1\n",
        "                except:\n",
        "                    word_pair_occurrence[(word1, word2)] = 1\n",
        "\n",
        "\n",
        "    for ind in tqdm(range(train_size+test_size)):\n",
        "        words = tokenize_sentences[ind]\n",
        "\n",
        "        q = []\n",
        "        # push the first (window_size) words into a queue\n",
        "        for i in range(min(window_size, len(words))):\n",
        "            q += [word_id_map[words[i]]]\n",
        "        # update the total number of the sliding windows\n",
        "        total_W += 1\n",
        "        # update the number of sliding windows that contain each word and word pair\n",
        "        update_word_and_word_pair_occurrence(q)\n",
        "\n",
        "        now_next_word_index = window_size\n",
        "        # pop the first word out and let the next word in, keep doing this until the end of the document\n",
        "        while now_next_word_index<len(words):\n",
        "            q.pop(0)\n",
        "            q += [word_id_map[words[now_next_word_index]]]\n",
        "            now_next_word_index += 1\n",
        "            # update the total number of the sliding windows\n",
        "            total_W += 1\n",
        "            # update the number of sliding windows that contain each word and word pair\n",
        "            update_word_and_word_pair_occurrence(q)\n",
        "\n",
        "    for word_pair in word_pair_occurrence:\n",
        "        i = word_pair[0]\n",
        "        j = word_pair[1]\n",
        "        count = word_pair_occurrence[word_pair]\n",
        "        word_freq_i = word_occurrence[i]\n",
        "        word_freq_j = word_occurrence[j]\n",
        "        pmi = log((count * total_W) / (word_freq_i * word_freq_j))\n",
        "        if pmi <=0:\n",
        "            continue\n",
        "        row.append(train_size + i)\n",
        "        col.append(train_size + j)\n",
        "        weight.append(pmi)\n",
        "        row.append(train_size + j)\n",
        "        col.append(train_size + i)\n",
        "        weight.append(pmi)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hynLnT3a33kW"
      },
      "source": [
        "### doc-word: Tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnSPqhg1lHps"
      },
      "outputs": [],
      "source": [
        "#get each word appears in which document\n",
        "word_doc_list = {}\n",
        "for word in word_list:\n",
        "    word_doc_list[word]=[]\n",
        "\n",
        "for i in range(len(tokenize_sentences)):\n",
        "    doc_words = tokenize_sentences[i]\n",
        "    unique_words = set(doc_words)\n",
        "    for word in unique_words:\n",
        "        exsit_list = word_doc_list[word]\n",
        "        exsit_list.append(i)\n",
        "        word_doc_list[word] = exsit_list\n",
        "\n",
        "#document frequency\n",
        "word_doc_freq = {}\n",
        "for word, doc_list in word_doc_list.items():\n",
        "    word_doc_freq[word] = len(doc_list)\n",
        "\n",
        "# term frequency\n",
        "doc_word_freq = {}\n",
        "\n",
        "for doc_id in range(len(tokenize_sentences)):\n",
        "    words = tokenize_sentences[doc_id]\n",
        "    for word in words:\n",
        "        word_id = word_id_map[word]\n",
        "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
        "        if doc_word_str in doc_word_freq:\n",
        "            doc_word_freq[doc_word_str] += 1\n",
        "        else:\n",
        "            doc_word_freq[doc_word_str] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6elPPFO_sXp"
      },
      "outputs": [],
      "source": [
        "for i in range(len(tokenize_sentences)):\n",
        "    words = tokenize_sentences[i]\n",
        "    doc_word_set = set()\n",
        "    for word in words:\n",
        "        if word in doc_word_set:\n",
        "            continue\n",
        "        j = word_id_map[word]\n",
        "        key = str(i) + ',' + str(j)\n",
        "        freq = doc_word_freq[key]\n",
        "        if i < train_size:\n",
        "            row.append(i)\n",
        "        else:\n",
        "            row.append(i + vocab_length)\n",
        "        col.append(train_size + j)\n",
        "        idf = log(1.0 * len(tokenize_sentences) / word_doc_freq[word_list[j]])\n",
        "        weight.append(freq * idf)\n",
        "        doc_word_set.add(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAr6ygKhWTc-"
      },
      "source": [
        "### doc-doc: jaccard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4-EH15oWWSX"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "if EDGE>=2:\n",
        "    tokenize_sentences_set = [set(s) for s in tokenize_sentences]\n",
        "    jaccard_threshold = 0.2\n",
        "    for i in tqdm(range(len(tokenize_sentences))):\n",
        "        for j in range(i+1, len(tokenize_sentences)):\n",
        "\n",
        "            #NOTE: RINA EDIT\n",
        "            #Jaccard distance is throwing an error when both sets are empty\n",
        "            if (len(tokenize_sentences_set[i]) == 0) & (len(tokenize_sentences_set[j]) == 0):\n",
        "              continue\n",
        "\n",
        "            jaccard_w = 1 - nltk.jaccard_distance(tokenize_sentences_set[i], tokenize_sentences_set[j])\n",
        "            if jaccard_w > jaccard_threshold:\n",
        "                if i < train_size:\n",
        "                    row.append(i)\n",
        "                else:\n",
        "                    row.append(i + vocab_length)\n",
        "                if j < train_size:\n",
        "                    col.append(j)\n",
        "                else:\n",
        "                    col.append(vocab_length + j)\n",
        "                weight.append(jaccard_w)\n",
        "                if j < train_size:\n",
        "                    row.append(j)\n",
        "                else:\n",
        "                    row.append(j + vocab_length)\n",
        "                if i < train_size:\n",
        "                    col.append(i)\n",
        "                else:\n",
        "                    col.append(vocab_length + i)\n",
        "                weight.append(jaccard_w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIkGgB2aZDk7"
      },
      "source": [
        "### Adjacent matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0O1Ucdhod9a"
      },
      "outputs": [],
      "source": [
        "import scipy.sparse as sp\n",
        "adj = sp.csr_matrix((weight, (row, col)), shape=(node_size, node_size))\n",
        "\n",
        "# build symmetric adjacency matrix\n",
        "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivyuexATkQFW"
      },
      "outputs": [],
      "source": [
        "def normalize_adj(adj):\n",
        "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo(), d_inv_sqrt\n",
        "\n",
        "adj, norm_item = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape).to(device)\n",
        "\n",
        "adj = sparse_mx_to_torch_sparse_tensor(adj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMgbhTstMSUA"
      },
      "source": [
        "## Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP9dqCskOrXT"
      },
      "outputs": [],
      "source": [
        "import scipy.sparse as sp\n",
        "def generate_features(word1_emb_type = None, word2_emb_type = None, emo_map = None, emo_matrix = None):\n",
        "  def preprocess_features(features):\n",
        "      \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
        "      rowsum = np.array(features.sum(1))\n",
        "      r_inv = np.power(rowsum, -1).flatten()\n",
        "      r_inv[np.isinf(r_inv)] = 0.\n",
        "      r_mat_inv = sp.diags(r_inv)\n",
        "      features = r_mat_inv.dot(features)\n",
        "      return features\n",
        "\n",
        "  if NODE == 0:\n",
        "      features = np.arange(node_size)\n",
        "      features = torch.FloatTensor(features).to(device)\n",
        "\n",
        "  elif NODE == 1:\n",
        "\n",
        "    from flair.embeddings import TransformerDocumentEmbeddings, TransformerWordEmbeddings\n",
        "    from flair.data import Sentence\n",
        "    doc_embedding = TransformerDocumentEmbeddings('bert-base-uncased', fine_tune=False)\n",
        "    word_embedding = TransformerWordEmbeddings('bert-base-uncased', layers='-1',subtoken_pooling=\"mean\")\n",
        "\n",
        "    sent_embs = []\n",
        "    word_embs = {}\n",
        "\n",
        "    for ind in tqdm(range(train_size+test_size)):\n",
        "        sent = tokenize_sentences[ind]\n",
        "        if len(sent) > 0:\n",
        "          sentence = Sentence(\" \".join(sent[:512]),use_tokenizer=False)\n",
        "          doc_embedding.embed(sentence)\n",
        "          sent_embs.append(sentence.get_embedding().tolist())\n",
        "          words = Sentence(\" \".join(sent[:512]),use_tokenizer=False)\n",
        "          word_embedding.embed(words)\n",
        "          for token in words:\n",
        "              word = token.text\n",
        "              embedding = token.embedding.tolist()\n",
        "              if word not in word_embs:\n",
        "                  word_embs[word] = embedding\n",
        "              else:\n",
        "                  word_embs[word] = np.minimum(word_embs[word], embedding)\n",
        "        else:\n",
        "          sent_embs.append([0] * 768)\n",
        "\n",
        "    word_embs_list = []\n",
        "    for word in word_list:\n",
        "      word_embs_list.append(word_embs[word])\n",
        "\n",
        "    features = sent_embs[:train_size] + word_embs_list + sent_embs[train_size:]\n",
        "\n",
        "    features = preprocess_features(sp.csr_matrix(features)).todense()\n",
        "    features = torch.FloatTensor(features).to(device)\n",
        "\n",
        "  elif NODE == 2 or NODE == 3:\n",
        "    if NODE == 2:\n",
        "      gloveModel, gloveDim = getGloveModel()\n",
        "\n",
        "      emb_map = gloveModel.wv.vocab\n",
        "      emb_weights = gloveModel\n",
        "      emb_dim = gloveDim\n",
        "    else:\n",
        "      emb_weights, emb_vocab_map = generate_custom_word_embeddings(word1_emb_type, word2_emb_type, emo_map, emo_matrix)\n",
        "      emb_weights = emb_weights.detach().numpy()\n",
        "      emb_weights_map = {k: emb_weights[i] for k, i in emb_vocab_map.items()}\n",
        "      emb_dim = emb_weights.shape[-1]\n",
        "\n",
        "    #SENTENCE EMBEDDINGS\n",
        "    sentence_emb = []\n",
        "    for idx in tqdm(range(train_size + test_size)):\n",
        "      sentence = tokenize_sentences[idx]\n",
        "      if len(sentence) > 0:\n",
        "        sentence_wEmb = []\n",
        "        for idx_token, token in enumerate(sentence):\n",
        "          if token in emb_weights_map:\n",
        "            sentence_wEmb.append(emb_weights_map[token])\n",
        "          else:\n",
        "            sentence_wEmb.append(np.zeros(emb_dim)) #UNKNOWN: set to 0s\n",
        "\n",
        "        sentence_emb.append(np.mean(sentence_wEmb, axis = 0)) #SENTENCE EMBEDDING: average\n",
        "      else:\n",
        "        #Append 0s if sentence is empty after tokenization\n",
        "        sentence_emb.append(np.zeros(emb_dim))\n",
        "    # sentence_emb = np.stack(sentence_emb)\n",
        "\n",
        "    #WORD EMBEDDINGS\n",
        "    word_embs_list = []\n",
        "    for word in word_list:\n",
        "      if word in emb_weights_map:\n",
        "        word_embs_list.append(emb_weights_map[word])\n",
        "      else:\n",
        "        word_embs_list.append(np.zeros(emb_dim))\n",
        "\n",
        "    features = sentence_emb[:train_size] + word_embs_list + sentence_emb[train_size:]\n",
        "    features = preprocess_features(sp.csr_matrix(features)).todense()\n",
        "    features = torch.FloatTensor(features).to(device)\n",
        "\n",
        "  else:\n",
        "    raise Exception(\"No node feature selected.\")\n",
        "\n",
        "  return features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "def align_custom_weights(custom_vocab_map, custom_weights, base_type = \"bert\"):\n",
        "\n",
        "  special_tokens = []\n",
        "  if base_type == \"bert\":\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    bertModel = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    bert_map = tokenizer.get_vocab()\n",
        "    bert_vocab = tokenizer.get_vocab().keys()\n",
        "    bert_weights = bertModel.embeddings.word_embeddings.weight.clone().detach()\n",
        "\n",
        "    #Determine out of vocabulary words, get word piece ids\n",
        "    oov = [w for w in custom_vocab_map.keys() if w not in bert_vocab]\n",
        "    oov_word_piece = [tokenizer(w)[\"input_ids\"][1:-1] for w in oov]\n",
        "    oov_word_piece_map = {w: oov_word_piece[i] for i, w in enumerate(oov)}\n",
        "\n",
        "    #Add oov to tokenizer vocab\n",
        "    new_vocab_len = tokenizer.add_tokens(oov)\n",
        "    final_vocab_map = tokenizer.get_vocab()\n",
        "    special_tokens = tokenizer.special_tokens_map.values()\n",
        "    print(\"Added tokens:\", new_vocab_len)\n",
        "    if len(oov) != new_vocab_len:\n",
        "      raise Exception(\"OOV does not match added vocab\")\n",
        "\n",
        "    #Resize bert_weights to add new tokens\n",
        "    base_weights = torch.cat((bert_weights, torch.zeros((new_vocab_len, bert_weights.shape[-1]))), dim = 0)\n",
        "\n",
        "    #Assign word piece mean as bert weights to new tokens\n",
        "    for w in oov:\n",
        "      idx = tokenizer.convert_tokens_to_ids(w.lower())\n",
        "      if idx == 100: #Unknown\n",
        "        raise Exception(\"Encountered unknown word:\", w)\n",
        "\n",
        "      base_weights[idx] = torch.mean(bert_weights[oov_word_piece_map[w]], dim = 0)\n",
        "\n",
        "    ##Clone base_weights to initialize weights to be concatenated\n",
        "    new_weights = base_weights.clone()\n",
        "\n",
        "    ##Set new weights\n",
        "    for w in custom_vocab_map:\n",
        "      ##Skip special tokens\n",
        "      if w in special_tokens:\n",
        "        continue\n",
        "\n",
        "      ##NOTE: lower to match bert implementation\n",
        "      idx = tokenizer.convert_tokens_to_ids(w.lower())\n",
        "      if idx == 100: #Unknown\n",
        "        raise Exception(\"Encountered unknown word:\", w)\n",
        "\n",
        "      if w in oov:\n",
        "        word_piece_ids = tokenizer(w, return_token_type_ids = False, return_attention_mask = False)[\"input_ids\"][1:-1]\n",
        "        base_weights[idx] = torch.mean(base_weights[word_piece_ids], dim = 0)\n",
        "\n",
        "      new_weights[idx] = custom_weights[custom_vocab_map[w]]\n",
        "  elif base_type == \"random\":\n",
        "    torch.manual_seed(0)\n",
        "    base_weights = torch.rand(custom_weights.shape)\n",
        "    final_vocab_map = custom_vocab_map\n",
        "    new_weights = custom_weights\n",
        "\n",
        "  else:\n",
        "    raise Exception(\"Base type (%s) not recognized.\" % (base_type))\n",
        "\n",
        "  #Concatenate weights per word\n",
        "  final_weights = torch.cat((base_weights, new_weights), dim = -1)\n",
        "\n",
        "  return final_vocab_map, final_weights"
      ],
      "metadata": {
        "id": "rY7MvJ5Negok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate Word Embedding variables\n",
        "def generate_custom_word_embeddings(word1_emb_type, word2_emb_type, emoWordEmb_map, emoWordEmb_weights):\n",
        "  if word2_emb_type == \"emo\":\n",
        "    print(\"Aligning word embeddings: %s and %s\" % (word1_emb_type, word2_emb_type))\n",
        "    final_vocab_map, final_weights = align_custom_weights(emoWordEmb_map, emoWordEmb_weights, base_type = word1_emb_type)\n",
        "    # final_seq_ids, final_vocab_map = generate_word_ids(clean_texts, FORCE_MAX_LENGTH, emb_type = \"custom\", word_id_map = final_vocab_map)\n",
        "  elif word2_emb_type is None:\n",
        "    if word1_emb_type != \"bert\":\n",
        "      # final_seq_ids, final_vocab_map = generate_word_ids(clean_texts, FORCE_MAX_LENGTH, emb_type = word1_emb_type, word_id_map = emoWordEmb_map)\n",
        "      final_vocab_map = emoWordEmb_map\n",
        "      final_weights = emoWordEmb_weights\n",
        "    else:\n",
        "      bertTokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "      bertModel = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "      # final_seq_ids, final_vocab_map = generate_word_ids(clean_texts, FORCE_MAX_LENGTH, emb_type = word1_emb_type)\n",
        "      final_vocab_map = bertTokenizer.get_vocab()\n",
        "      final_weights = bertModel.embeddings.word_embeddings.weight.detach()\n",
        "  else:\n",
        "    raise Exception(\"Not yet implemented\")\n",
        "  print(\"Word1 emb type:\", word1_emb_type)\n",
        "  print(\"Word2 emb type:\", word2_emb_type)\n",
        "  print(\"Final weight shape:\", final_weights.shape)\n",
        "\n",
        "  #Add random weights if new tokens (ie. unknown, pad) are added\n",
        "  # if len(final_vocab_map.keys()) > final_weights.shape[0]:\n",
        "  #   print(\"Adding weights...\")\n",
        "  #   add_dim = len(final_vocab_map.keys()) - final_weights.shape[0]\n",
        "  #   rand_weights = torch.rand(size = (add_dim, final_weights.shape[1]))\n",
        "  #   final_weights = torch.cat((final_weights, rand_weights), dim = 0)\n",
        "\n",
        "  return final_weights, final_vocab_map"
      ],
      "metadata": {
        "id": "Sa_qoSw4c0wL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdx6RrUvjbF0"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39Kj8NQujiDH"
      },
      "source": [
        "## GCN Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNVkA-h7b3sP"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features,  drop_out = 0, activation=None, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.zeros(1, out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters(in_features, out_features)\n",
        "        self.dropout = torch.nn.Dropout(drop_out)\n",
        "        self.activation =  activation\n",
        "\n",
        "    def reset_parameters(self,in_features, out_features):\n",
        "        stdv = np.sqrt(6.0/(in_features+out_features))\n",
        "        # stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        # if self.bias is not None:\n",
        "        #     torch.nn.init.zeros_(self.bias)\n",
        "            # self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "    def forward(self, input, adj, feature_less = False):\n",
        "        if feature_less:\n",
        "            support = self.weight\n",
        "            support = self.dropout(support)\n",
        "        else:\n",
        "            input = self.dropout(input)\n",
        "            support = torch.mm(input, self.weight)\n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            output = output + self.bias\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k57M4sz4s4Md"
      },
      "source": [
        "## GCN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJ-ZQuMzs5tZ"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout, n_layers = 2):\n",
        "        super(GCN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.gc_list = []\n",
        "        if n_layers >= 2:\n",
        "            self.gc1 = GraphConvolution(nfeat, nhid, dropout, activation = nn.ReLU())\n",
        "            self.gc_list = nn.ModuleList([GraphConvolution(nhid, nhid, dropout, activation = nn.ReLU()) for _ in range(self.n_layers-2)])\n",
        "            self.gcf = GraphConvolution(nhid, nclass, dropout)\n",
        "        else:\n",
        "            self.gc1 = GraphConvolution(nfeat, nclass, dropout)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        if self.n_layers>=2:\n",
        "            x = self.gc1(x, adj, feature_less = True)\n",
        "            for i in range(self.n_layers-2):\n",
        "                x = self.gc_list[i](x,adj)\n",
        "            x = self.gcf(x,adj)\n",
        "        else:\n",
        "            x = self.gc1(x, adj, feature_less = True)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmhOG1yG--Ji"
      },
      "outputs": [],
      "source": [
        "def cal_accuracy(predictions,labels):\n",
        "    pred = torch.argmax(predictions,-1).cpu().tolist()\n",
        "    lab = labels.cpu().tolist()\n",
        "    cor = 0\n",
        "    for i in range(len(pred)):\n",
        "        if pred[i] == lab[i]:\n",
        "            cor += 1\n",
        "    return cor/len(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEE4JxeUthCb"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIxII4QoticA"
      },
      "source": [
        "## Initialize model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdNsgxMG-Wwu"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# model = GCN(nfeat=node_size, nhid=HIDDEN_DIM, nclass=num_class, dropout=DROP_OUT,n_layers=NUM_LAYERS).to(device)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T98r4qZuuFyn"
      },
      "source": [
        "## Training and Validating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QC7u3Jn2uIu4"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def train_model(show_result = True):\n",
        "    val_loss = []\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        t = time.time()\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output= model(features, adj)\n",
        "        loss_train = criterion(output[idx_train], labels[idx_train])\n",
        "        acc_train = cal_accuracy(output[idx_train], labels[idx_train])\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        output = model(features, adj)\n",
        "\n",
        "        loss_val = criterion(output[idx_val], labels[idx_val])\n",
        "        val_loss.append(loss_val.item())\n",
        "        acc_val = cal_accuracy(output[idx_val], labels[idx_val])\n",
        "        if show_result:\n",
        "            print(  'Epoch: {:04d}'.format(epoch+1),\n",
        "                    'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "                    'acc_train: {:.4f}'.format(acc_train),\n",
        "                    'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "                    'acc_val: {:.4f}'.format(acc_val),\n",
        "                    'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "        if epoch > EARLY_STOPPING and np.min(val_loss[-EARLY_STOPPING:]) > np.min(val_loss[:-EARLY_STOPPING]) :\n",
        "            if show_result:\n",
        "                print(\"Early Stopping...\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_train_val(train_pro=0.9):\n",
        "    real_train_size = int(train_pro*train_size)\n",
        "    val_size = train_size-real_train_size\n",
        "\n",
        "    idx_train = np.random.choice(train_size, real_train_size,replace=False)\n",
        "    idx_train.sort()\n",
        "    idx_val = []\n",
        "    pointer = 0\n",
        "    for v in range(train_size):\n",
        "        if pointer<len(idx_train) and idx_train[pointer] == v:\n",
        "            pointer +=1\n",
        "        else:\n",
        "            idx_val.append(v)\n",
        "    idx_test = np.arange(train_size+vocab_length, node_size)\n",
        "    return idx_train, idx_val, idx_test"
      ],
      "metadata": {
        "id": "6R8xeMkJxG5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model():\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "\n",
        "    return test_labels, output[idx_test].cpu().detach().numpy()"
      ],
      "metadata": {
        "id": "rL81pB9nxG5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQwlWq6dyYJm"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "def evaluate_output(outputs, targets, targetLabels, session_name = \"\", show_results = True, multiLabel = False, return_results = False):\n",
        "\n",
        "    if multiLabel:\n",
        "      outputs = np.array(outputs) >= 0.5\n",
        "    else:\n",
        "      outputs = np.argmax(outputs, axis = 1)\n",
        "\n",
        "    accuracy = accuracy_score(targets, outputs)\n",
        "    f1_score_micro = f1_score(targets, outputs, average='micro')\n",
        "    f1_score_macro = f1_score(targets, outputs, average='macro')\n",
        "    f1_score_weighted = f1_score(targets, outputs, average=\"weighted\")\n",
        "\n",
        "    if show_results:\n",
        "      print()\n",
        "      print(\"=\" * 50)\n",
        "      print(session_name)\n",
        "      print(\"=\" * 50)\n",
        "      print(\"Accuracy Score: %.4f\" % (accuracy))\n",
        "      print(\"F1 Score (Micro): %.4f\" % (f1_score_micro))\n",
        "      print(\"F1 Score (Macro): %.4f\" % (f1_score_macro))\n",
        "      print(\"F1 Score (Weighted): %.4f\" % (f1_score_weighted))\n",
        "\n",
        "      if multiLabel:\n",
        "        ham_loss = hamming_loss(targets, outputs)\n",
        "        print(\"Hamming Loss: %.4f\" % (ham_loss))\n",
        "\n",
        "\n",
        "      print(classification_report(targets, outputs, target_names = targetLabels, digits = 4))\n",
        "\n",
        "    if return_results:\n",
        "      results = {\"Accuracy\": accuracy,\n",
        "              \"F1_Micro\": f1_score_micro,\n",
        "              \"F1_Macro\": f1_score_macro,\n",
        "              \"F1_Weighted\": f1_score_weighted,\n",
        "              \"Class Precision\": precision_score(targets, outputs, average = None),\n",
        "              \"Class Recall\": recall_score(targets, outputs, average = None),\n",
        "              \"Class F1\": f1_score(targets, outputs, average = None)\n",
        "      }\n",
        "\n",
        "      return results"
      ],
      "metadata": {
        "id": "pjnwvyIDt2Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_predictions(path, ids, texts, goldLabels, classes, probs, preds):\n",
        "  #Save predictions\n",
        "  os.makedirs(os.path.dirname(path), exist_ok = True)\n",
        "  saveOutput = pd.DataFrame({\"Text\": texts,\n",
        "                             \"Label\": goldLabels},\n",
        "                             index = ids)\n",
        "\n",
        "  for i, c in enumerate(classes):\n",
        "    saveOutput[c] = probs[:, i]\n",
        "\n",
        "  saveOutput[\"Prediction\"] = preds\n",
        "  saveOutput = saveOutput.sort_index()\n",
        "  saveOutput.to_csv(path)"
      ],
      "metadata": {
        "id": "FugEnF4BwSWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tuning"
      ],
      "metadata": {
        "id": "Oc26jk8Cnwei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "def objective(trial):\n",
        "  #Generate model\n",
        "\n",
        "  tune_dropout = trial.suggest_categorical(\"dropout\", [0.01, 0.05, 0.1, 0.5])\n",
        "  tune_layers = 2 #trial.suggest_int(\"num_layers\", 2, 5)\n",
        "  tune_hidden = 200 #trial.suggest_int(\"num_hidden\", 100, 500, step = 100)\n",
        "\n",
        "  tune_model = GCN(nfeat=node_size, nhid=tune_hidden, nclass=num_class, dropout=tune_dropout, n_layers=tune_layers).to(device)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  #Generate optimizers\n",
        "  tune_lr = trial.suggest_float(\"lr\", 0.01, 0.05, step = 0.01)\n",
        "  tune_decay = trial.suggest_categorical(\"weight_decay\", [0, 0.005, 0.05])\n",
        "  optimizer = optim.Adam(tune_model.parameters(), lr=tune_lr, weight_decay=tune_decay)\n",
        "\n",
        "  #Prepare dataset\n",
        "  # idx_train, idx_val, _ = generate_train_val()\n",
        "\n",
        "  #Training\n",
        "  val_loss = []\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    t = time.time()\n",
        "    tune_model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output= tune_model(features, adj)\n",
        "    loss_train = criterion(output[idx_train], labels[idx_train])\n",
        "    acc_train = cal_accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    tune_model.eval()\n",
        "    output = tune_model(features, adj)\n",
        "\n",
        "    loss_val = criterion(output[idx_val], labels[idx_val])\n",
        "    val_loss.append(loss_val.item())\n",
        "    acc_val = cal_accuracy(output[idx_val], labels[idx_val])\n",
        "\n",
        "    if epoch > EARLY_STOPPING and np.min(val_loss[-EARLY_STOPPING:]) > np.min(val_loss[:-EARLY_STOPPING]) :\n",
        "        # if show_result:\n",
        "        #     print(\"Early Stopping...\")\n",
        "        break\n",
        "\n",
        "\n",
        "    #Record accuracy\n",
        "    trial.report(acc_val, epoch)\n",
        "\n",
        "    # Handle pruning based on the intermediate value.\n",
        "    if trial.should_prune():\n",
        "        raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "  return acc_val\n"
      ],
      "metadata": {
        "id": "SbRnlCxZnyE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mental Health Classification"
      ],
      "metadata": {
        "id": "ddETGUC5_Ygi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load custom embeddings\n",
        "with open(mmemogFile, \"rb\") as file:\n",
        "  content = pickle.load(file)\n",
        "\n",
        "emoWordEmb_map = content[\"vocab_map\"]\n",
        "emoWordEmb_weights = content[\"weights\"]"
      ],
      "metadata": {
        "id": "frquQ4J54yIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = generate_features(\"bert\", \"emo\", emo_map = emoWordEmb_map, emo_matrix = emoWordEmb_weights)\n",
        "features.shape"
      ],
      "metadata": {
        "id": "YxDNLRTon7Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tune"
      ],
      "metadata": {
        "id": "wwfOPkzyPHTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx_train, idx_val, idx_test = generate_train_val()\n",
        "study = optuna.create_study(direction = \"maximize\")\n",
        "study.enqueue_trial({\"num_hidden\": 200,\n",
        "                     \"num_layers\": 2,\n",
        "                     \"dropout\": 0.5,\n",
        "                     \"lr\": 0.02,\n",
        "                     \"weight_decay\": 0})\n",
        "study.optimize(objective, n_trials = 50)"
      ],
      "metadata": {
        "id": "ma8HB7kiPHTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_trial = study.best_trial\n",
        "best_params = best_trial.params\n",
        "print(\"BEST:\", best_trial.value)\n",
        "print(\"Params:\")\n",
        "for key, value in best_params.items():\n",
        "  print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "hidden_dim = 200\n",
        "dropout = best_params[\"dropout\"]\n",
        "num_layers = 2\n",
        "learn_rate = best_params[\"lr\"]\n",
        "weight_decay = best_params[\"weight_decay\"]"
      ],
      "metadata": {
        "id": "vqyr2Bh8PHTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1-Run"
      ],
      "metadata": {
        "id": "Yw3MydPZPHTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx_train, idx_val, idx_test = generate_train_val()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = GCN(nfeat=node_size, nhid=hidden_dim, nclass=num_class, dropout=dropout,n_layers=num_layers).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learn_rate, weight_decay=weight_decay)\n",
        "train_model()\n",
        "\n",
        "gold, probs = test_model()\n",
        "\n",
        "preds = np.argmax(probs, axis = -1)\n",
        "results = evaluate_output(probs, gold, lEnc.classes_.astype(str), session_name, return_results = True)\n",
        "save_predictions(save_output_path,\n",
        "                np.array(idx_test),\n",
        "                original_test_sentences,\n",
        "                lEnc.inverse_transform(gold),\n",
        "                lEnc.classes_,\n",
        "                np.array(probs),\n",
        "                lEnc.inverse_transform(preds))"
      ],
      "metadata": {
        "id": "eYto1N76PHTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##10-Run"
      ],
      "metadata": {
        "id": "upFGP7bcPHTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = []\n",
        "\n",
        "for t in tqdm(range(10)):\n",
        "  idx_train, idx_val, idx_test = generate_train_val()\n",
        "\n",
        "  model = GCN(nfeat=node_size, nhid=hidden_dim, nclass=num_class, dropout=dropout,n_layers=num_layers).to(device)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learn_rate, weight_decay=weight_decay)\n",
        "  train_model(show_result = False)\n",
        "\n",
        "  gold, probs = test_model()\n",
        "\n",
        "  preds = np.argmax(probs, axis = -1)\n",
        "  results = evaluate_output(probs, gold, lEnc.classes_.astype(str), session_name, show_results = False, return_results = True)\n",
        "\n",
        "  test_results.append(results)"
      ],
      "metadata": {
        "id": "EWgACGrvPHTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Collate results\n",
        "result_average = {k: [] for k in test_results[0].keys()}\n",
        "for r in test_results:\n",
        "  for k in result_average:\n",
        "    result_average[k].append(r[k])\n",
        "\n",
        "#Average results\n",
        "print(\"=\" * 50)\n",
        "print(\"10-run average\")\n",
        "print(\"=\" * 50)\n",
        "tab = []\n",
        "tab_sd = []\n",
        "header = []\n",
        "for k, item in result_average.items():\n",
        "  if k.split(\" \")[0] == \"Class\":\n",
        "    header.append(k.split(\" \")[1])\n",
        "    tab.append(np.mean(item, axis = 0))\n",
        "    tab_sd.append(np.std(item, axis = 0))\n",
        "  else:\n",
        "    print(\"%s: %.4f ± %.4f\" % (k, np.mean(item, axis = 0), np.std(item, axis = 0)))\n",
        "# print(\"\\n\", tabulate(np.hstack(([[h] for h in header], np.round(np.array(tab), 4))), headers = lEnc.classes_))\n",
        "\n",
        "class_sd = np.reshape([\"%.4f ± %.4f\" % (z, a) for x, y in zip(tab, tab_sd) for z, a in zip(x, y)], newshape=(3, num_class))\n",
        "print(\"\\n\", tabulate(np.hstack(([[h] for h in header], class_sd)), headers = lEnc.classes_))"
      ],
      "metadata": {
        "id": "ah0j92-ePHTE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "6K9dWTv5I07_",
        "a2W7wKTBfa71",
        "eyfCxMUJeEQ4",
        "ZMkEBxr6fMQi",
        "hobYcJ5OX5oT",
        "g0o8wcXgrTiD",
        "znJ7Grz7fQ2L",
        "QESQPT88AqsI",
        "hynLnT3a33kW",
        "FAr6ygKhWTc-",
        "uIkGgB2aZDk7",
        "pMgbhTstMSUA",
        "pdx6RrUvjbF0",
        "39Kj8NQujiDH",
        "k57M4sz4s4Md",
        "bIxII4QoticA",
        "OQwlWq6dyYJm",
        "rddrtd6Bvmhb",
        "ddETGUC5_Ygi",
        "dwFYr_QzPOZx"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}